{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEodo3D9I79E"
   },
   "source": [
    "We consider binary classification model with regularized logistic regression loss $$\\ell(\\theta, \\{x_i, \\xi_i\\}_{i = 1, \\ldots, n}) = \\frac{1}{n} \\sum_{i=1}^n \\ln(1 + \\exp(-x_i^\\top\\theta \\xi_i) + \\frac{\\lambda}{2}\\|\\theta\\|^2,$$\n",
    "where $x_i$ is the normalized feature vector and label $\\xi_i \\in \\{-1, 1\\}$ of data point $i$ respectively. The gradient of $\\ell$ is $$\\frac{1}{n}\\sum_{i=1}^n (-\\xi_i x_i) \\cdot \\frac{1}{1+\\exp(x_i^\\top \\theta x_i)} + \\lambda \\theta.$$\n",
    "\n",
    "Note that the\n",
    "\n",
    "\n",
    "This code base is no longer updated, should be deleted some time later.\n",
    "\n",
    "The probability of data point $x_i$ has label $+1$ is $\\frac{1}{1 + \\exp(-x_i^\\top \\theta)}$, so the decision boundary is $x_i^\\top \\theta =0$, if $x_i^\\top \\theta > 0$, then $x_i$ should be classified as $+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXuSIADOE8ea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csgraph\n",
    "from numpy import linalg as la\n",
    "from copy import deepcopy\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### data preparation\n",
    "def fashion_mnist():\n",
    "  # fetching data takes time, so do it once\n",
    "  if not os.path.exists('./datasets'):\n",
    "    os.mkdir('./datasets')\n",
    "  if os.path.exists('./datasets/fashion_mnist.npz'):\n",
    "    print('Fashion-MNIST exists')\n",
    "    data = np.load('./datasets/fashion_mnist.npz', allow_pickle=True)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "  else:\n",
    "    print('Downloading fashion_mnist')\n",
    "    X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True, as_frame=False)\n",
    "    np.savez_compressed('./datasets/fashion_mnist', X=X, y=y)\n",
    "    print('fashion_mnist downloaded')\n",
    "\n",
    "  # binary labels, train test datasets split, 5k/2k for each class\n",
    "  class0 = '1' #pullover\n",
    "  class1 = '4' #coat\n",
    "  X0 = X[y == class0]\n",
    "  X1 = X[y == class1]\n",
    "  X_train = np.concatenate((X0[:5000], X1[:5000]), axis=0)\n",
    "  X_test = np.concatenate((X0[5000:], X1[5000:]), axis=0)\n",
    "  y_train = np.concatenate((np.ones(5000), -np.ones(5000)))\n",
    "  y_test = np.concatenate((np.ones(2000), -np.ones(2000)))\n",
    "\n",
    "  # random shuffling the dataset\n",
    "  perm1 = np.random.permutation(X_train.shape[0])\n",
    "  perm2 = np.random.permutation(X_test.shape[0])\n",
    "  X_train = X_train[perm1]\n",
    "  y_train = y_train[perm1]\n",
    "  X_test = X_test[perm2]\n",
    "  y_test = y_test[perm2]\n",
    "\n",
    "  # data scaling\n",
    "  X_train = X_train / np.linalg.norm(X_train, axis=1)[:, None]\n",
    "  X_test = X_test / np.linalg.norm(X_test, axis=1)[:, None]\n",
    "  return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "### graphs\n",
    "# geometric graph\n",
    "def geometric_graph_undirected(num_agents, max_distance):\n",
    "  distance_nodes = np.zeros((num_agents, num_agents))\n",
    "  connected = False\n",
    "  while not connected:\n",
    "      ## each row is the coordinate of a node\n",
    "      coordinate_nodes = np.random.uniform(0, 1, (num_agents, 2))\n",
    "      for i in range(num_agents):\n",
    "          for j in range(num_agents):\n",
    "              distance_nodes[i][j] \\\n",
    "                  = la.norm(coordinate_nodes[i] - coordinate_nodes[j])\n",
    "      ## if distance less than max_distance then connect\n",
    "      network = (distance_nodes <= max_distance) * 1 - np.identity(num_agents)\n",
    "      if la.matrix_power(network, num_agents - 1).all() > 0:\n",
    "          connected = True\n",
    "  return network\n",
    "\n",
    "# connected cycle with certain degree\n",
    "def connected_cycle(num_agents, deg):\n",
    "  network = np.zeros((num_agents, num_agents))\n",
    "  for i in range(num_agents):\n",
    "     for j in range(i+1, i+deg+1):\n",
    "       network[i, j%num_agents] = 1\n",
    "  return network + network.T\n",
    "\n",
    "# network = connected_cycle(5, 2)\n",
    "# L = csgraph.laplacian(network)\n",
    "# nx_graph = nx.from_numpy_array(network)\n",
    "# # nx.nx_agraph.graphviz_layout(nx_graph)\n",
    "# nx.draw(nx_graph, nx.spring_layout(nx_graph), with_labels=True, node_color='y')\n",
    "# plt.savefig('network.pdf', dpi=1200)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "### mixing matrices\n",
    "# given a connectivity matrix, compute some mixing matrix as weight matrix\n",
    "def uniform_weight(network):\n",
    "  # network is an adjacency matrix that consists of 0 and 1, symmetric\n",
    "  network_self = network + np.eye(network.shape[0])\n",
    "  deg_vec = np.sum(network_self, axis=1)\n",
    "  return (network_self.T * (1/deg_vec)).T\n",
    "\n",
    "# ref: 'A scheme for robust distributed sensor fusion based on average consensus'\n",
    "def metropolis_weight(network):\n",
    "  deg_vec = np.sum(network, axis=1)\n",
    "  num_nodes = len(deg_vec)\n",
    "  weights = np.zeros((num_nodes, num_nodes))\n",
    "  for i in range(num_nodes):\n",
    "    for j in range(num_nodes):\n",
    "      if network[i, j] == 1 and i != j:\n",
    "        weights[i, j] = 1 / (1 + np.maximum(deg_vec[i], deg_vec[j]))\n",
    "    weights[i, i] = 1 - np.sum(weights[i])\n",
    "  return weights\n",
    "\n",
    "\n",
    "### problems\n",
    "class LRGradientAttack():\n",
    "  '''\n",
    "  Each agent minimizes the same regularized logistic regression loss defined on\n",
    "  the same dataset. Data and topologies are processed in other modules, this class\n",
    "  is dedicated to problem model, and attack model.\n",
    "  '''\n",
    "  def __init__(self, network, weights, attacked_agents, attack_type, data):\n",
    "    self.network = network #an adjency matrix without weights\n",
    "    self.weights = weights\n",
    "    self.num_agents = network.shape[0]\n",
    "    self.attacked_agents = attacked_agents #a list of indices\n",
    "    self.attack_type = attack_type #choose a typical type of attack\n",
    "    self.data = data #(X_train, y_train, X_test, y_test)\n",
    "    self.theta_len = data[0].shape[1]\n",
    "    self.theta = np.random.rand(self.num_agents, self.theta_len)\n",
    "    self.lamb = 0.1\n",
    "    self.paras = self.prob_paras()\n",
    "\n",
    "  def prob_paras(self):\n",
    "    # compute the smoothness, convexity paras\n",
    "    q = self.data[0].T @ self.data[0] / len(self.data[0])\n",
    "    L_F = max(abs(np.linalg.eigvals(q)))/4\n",
    "    L = L_F + self.lamb\n",
    "    kappa = L / self.lamb\n",
    "    return L, kappa\n",
    "\n",
    "  def stochastic_grad(self, mini_batch):\n",
    "    # random shuffling the dataset and iterate over it\n",
    "    # mini_batch: (num_agents x mini_batch indices)\n",
    "    sgd = np.zeros((self.num_agents, self.theta_len))\n",
    "    for i in range(self.num_agents):\n",
    "      x_sample = self.data[0][mini_batch[i], :]\n",
    "      y_sample = self.data[1][mini_batch[i]]\n",
    "      num_sample = x_sample.shape[0]\n",
    "      #  attacked agents\n",
    "      if i in self.attacked_agents:\n",
    "        if self.attack_type == 'flip':\n",
    "          # np.random.shuffle(y_sample)\n",
    "          y_sample = -y_sample\n",
    "          temp = x_sample @ self.theta[i] * y_sample\n",
    "          temp = - y_sample * (1 / (1 + np.exp(temp)))\n",
    "          sgd[i] = np.average((x_sample.T * temp).T, axis=0) \\\n",
    "                + self.lamb*self.theta[i]\n",
    "        if self.attack_type == 'persistent':\n",
    "          sgd[i] = np.ones(x_sample.shape[1])\n",
    "        if self.attack_type == 'allone':\n",
    "          y_sample = np.ones(len(y_sample))\n",
    "          temp = x_sample @ self.theta[i] * y_sample\n",
    "          temp = - y_sample * (1 / (1 + np.exp(temp)))\n",
    "          sgd[i] = np.average((x_sample.T * temp).T, axis=0) \\\n",
    "                + self.lamb*self.theta[i]\n",
    "      else:\n",
    "        temp1 = x_sample @ self.theta[i] * y_sample\n",
    "        temp2 = - (1 / (1 + np.exp(temp1))) * y_sample\n",
    "        sgd[i] = np.average((x_sample.T * temp2).T, axis=0) \\\n",
    "                + self.lamb*self.theta[i]\n",
    "    return sgd\n",
    "\n",
    "  def sgd_step(self, mini_batch, step_size, clip, vr=False, prev=None):\n",
    "    sgd = self.stochastic_grad(mini_batch)\n",
    "    sgd_norm = la.norm(sgd, axis=1)\n",
    "    clip_coef = np.where(sgd_norm <= clip, 1, 1/sgd_norm)\n",
    "    sgd_clipped = (sgd.T * clip_coef).T\n",
    "    if not vr:\n",
    "      self.theta = self.weights @ (self.theta - step_size*sgd_clipped)\n",
    "    else:\n",
    "      self.theta = 0\n",
    "\n",
    "  def vr_step(self, mini_batch, step_size, clip, prev_dir):\n",
    "    sgd = self.stochastic_grad(mini_batch)\n",
    "    sgd_norm = la.norm(sgd, axis=1)\n",
    "    clip_coef = np.where(sgd_norm <= clip, 1, 1/sgd_norm)\n",
    "    sgd_clipped = (sgd.T * clip_coef).T\n",
    "\n",
    "\n",
    "  def train_loss(self):\n",
    "    loss = np.zeros(self.num_agents)\n",
    "    for i in range(self.num_agents):\n",
    "      z = - self.data[0] @ self.theta[i] * self.data[1]\n",
    "      loss[i] = np.average(1 + np.exp(z))/len(self.data[1]) \\\n",
    "                + self.lamb*np.linalg.norm(self.theta[i])\n",
    "    return loss\n",
    "\n",
    "  def acc(self):\n",
    "    acc = np.zeros((self.num_agents, 2))\n",
    "    for i in range(self.num_agents):\n",
    "      z_train = self.data[0] @ self.theta[i]\n",
    "      pred_train = np.where(z_train >= 0, 1, -1)\n",
    "      acc[i, 0] = 1 - 0.5 * np.linalg.norm(pred_train-self.data[1], 1)/len(self.data[1])\n",
    "      z_test = self.data[2] @ self.theta[i]\n",
    "      pred_test = np.where(z_test >= 0, 1, -1)\n",
    "      acc[i, 1] = 1 - 0.5 * np.linalg.norm(pred_test-self.data[3], 1)/len(self.data[3])\n",
    "    return acc\n",
    "\n",
    "  def consensus_err(self):\n",
    "    return np.sum(np.var(self.theta, axis=0))\n",
    "\n",
    "\n",
    "### utils\n",
    "def epoch_random_batches(num_agents, epoch_size):\n",
    "  mini_batches = np.array([np.arange(epoch_size) for i in range(num_agents)])\n",
    "  for i in range(num_agents):\n",
    "    np.random.shuffle(mini_batches[i])\n",
    "  return mini_batches\n",
    "\n",
    "### run experiments\n",
    "# network = connected_cycle(5, 2)\n",
    "# mixing = uniform_weight(network)\n",
    "# mixing = metropolis_weight(network)\n",
    "# print(mixing)\n",
    "# print(network)\n",
    "\n",
    "\n",
    "### problem setup\n",
    "np.random.seed(0)\n",
    "data = fashion_mnist()\n",
    "network = connected_cycle(15, 3)\n",
    "num_agents = network.shape[0]\n",
    "weights = metropolis_weight(network)\n",
    "attacked_agents = []\n",
    "# attack_type = 'persistent'\n",
    "attack_type = 'allone'\n",
    "malr = LRGradientAttack(network, weights, attacked_agents, attack_type, data)\n",
    "train_size = 5000\n",
    "num_epoch = 10\n",
    "batch_size = 200\n",
    "num_batches = int(train_size/batch_size)\n",
    "vr = True\n",
    "\n",
    "### stepsize and clipping parameters\n",
    "alpha0 = 10\n",
    "gamma0 = 100\n",
    "eta0 = 10\n",
    "phi = 1\n",
    "tau_alpha = 1\n",
    "tau_gamma = 1\n",
    "tau_eta = 1\n",
    "stepsize = lambda t: alpha0 / (t+phi)**tau_alpha\n",
    "clipbound = lambda t: gamma0 / (t+phi)**tau_gamma\n",
    "eta = lambda t: eta0 / (t+1)**tau_eta\n",
    "###\n",
    "\n",
    "### train\n",
    "t = 0\n",
    "train_losses = np.zeros((num_agents, num_epoch+1))\n",
    "test_acc = np.zeros((num_agents, num_epoch+1))\n",
    "consensus_errs = np.zeros(num_epoch+1)\n",
    "for epoch in range(num_epoch):\n",
    "  train_losses[:, epoch] = malr.train_loss()\n",
    "  test_acc[:, epoch] = malr.acc()[:, 1]\n",
    "  consensus_errs[epoch] = malr.consensus_err()\n",
    "  mini_batch_all = epoch_random_batches(network.shape[0], train_size)\n",
    "  for batch_idx in range(int(train_size/batch_size)):\n",
    "    batch_start = batch_idx * batch_size\n",
    "    mini_batch = mini_batch_all[:, batch_start:(batch_start+batch_size)]\n",
    "    if vr:\n",
    "      malr.sgd_step(mini_batch, stepsize(t), clipbound(t))\n",
    "    else:\n",
    "      malr.sgd_step(mini_batch, stepsize(t), clipbound(t))\n",
    "    t += 1\n",
    "train_losses[:, epoch+1] = malr.train_loss()\n",
    "test_acc[:, epoch+1] = malr.acc()[:, 1]\n",
    "consensus_errs[epoch+1] = malr.consensus_err()\n",
    "print(num_agents * 1/(1+malr.paras[1]))\n",
    "### train ends\n",
    "\n",
    "### plots\n",
    "# cls = ['b', 'g', 'm', 'r']\n",
    "# linestyles = ['solid', 'dotted', 'dashed', 'dashdot']\n",
    "# markers = ['^', '+', 'x', 'o']\n",
    "# plots = []\n",
    "# plt.clf()\n",
    "# plt.rc('text', usetex=True)\n",
    "# plt.rc('font', family='monospace')\n",
    "# lbs = [r'$\\mathbf{w}_i$, unattacked', r'$\\mathbf{x}_i$, unattacked',\n",
    "#        r'$\\mathbf{x}_i$, attacked', r'$\\mathbf{w}_i$, attacked']\n",
    "# accs = [acc_nonbz_mean, testacc_nonbz_mean, testacc_bzmean, acc_bzmean]\n",
    "# for i in range(len(accs)):\n",
    "#   plots.append(plt.plot(np.arange(len(accs[i]))*50, accs[i],\n",
    "#                         c=cls[i], marker=markers[i], linestyle = linestyles[i])[0])\n",
    "# plt.xlabel(\"SGD step count\", fontsize=16)\n",
    "# plt.ylabel(\"accuracy\", fontsize=16)\n",
    "# # plt.title('Average accuracies of $\\{\\mathbf{x}_i\\}$ and $\\{\\mathbf{w}_i\\}$', fontsize=16)\n",
    "# plt.legend(handles=plots, labels=lbs, loc='best')\n",
    "# plt.savefig('accuracies1.pdf', dpi=1200)\n",
    "# plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(np.arange(num_epoch+1), np.average(train_losses, axis=0))\n",
    "# plt.plot(np.arange(num_epoch), np.average(train_acc, axis=0))\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "# plt.plot(np.arange(num_epoch+1), consensus_errs)\n",
    "plt.plot(np.arange(num_epoch+1), np.average(test_acc, axis=0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
